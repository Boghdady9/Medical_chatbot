{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30823,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Load",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "!pip install -U bitsandbytes\n!pip install evaluate peft\n!pip install rouge_score trl\n!pip install -U accelerate\n",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "from transformers import  TrainingArguments\nimport torch \nimport time \nimport evaluate  ## for calculating rouge score\nimport pandas as pd\nimport numpy as np",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-21T12:43:51.305559Z",
     "iopub.execute_input": "2024-12-21T12:43:51.305881Z",
     "iopub.status.idle": "2024-12-21T12:43:51.402013Z",
     "shell.execute_reply.started": "2024-12-21T12:43:51.305859Z",
     "shell.execute_reply": "2024-12-21T12:43:51.401413Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": "## Load Model in full precesion",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nimport torch\n\nbase_model=\"Qwen/Qwen2.5-1.5B-Instruct\"\n\nnf4_config = BitsAndBytesConfig(\n   load_in_4bit=True,\n   bnb_4bit_quant_type=\"nf4\",\n   bnb_4bit_use_double_quant=True,\n   bnb_4bit_compute_dtype=torch.bfloat16\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n)\n\ntokenizer = AutoTokenizer.from_pretrained(base_model)\ntokenizer.pad_token = tokenizer.eos_token\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-21T12:39:01.525346Z",
     "iopub.execute_input": "2024-12-21T12:39:01.525657Z",
     "iopub.status.idle": "2024-12-21T12:41:19.474733Z",
     "shell.execute_reply.started": "2024-12-21T12:39:01.525632Z",
     "shell.execute_reply": "2024-12-21T12:41:19.474014Z"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a50f3f45ff994334bdad8badf6c5361c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5f373a905bd04de283e8786c57e5fa9b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "56106d3e42cf49008fa3578c0178a095"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "75b298488d13435fb9a7724930be18fe"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b758351bd66f4144870115baf70e7517"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d981108f821a4d55b849fe7c17823267"
      }
     },
     "metadata": {}
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": "## Load Data",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from datasets import load_dataset\n\nds = load_dataset(\"BoghdadyJR/chatbot_medical\")\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-21T12:41:19.475748Z",
     "iopub.execute_input": "2024-12-21T12:41:19.476300Z",
     "iopub.status.idle": "2024-12-21T12:41:27.476164Z",
     "shell.execute_reply.started": "2024-12-21T12:41:19.476266Z",
     "shell.execute_reply": "2024-12-21T12:41:27.475286Z"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "README.md:   0%|          | 0.00/609 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "15791e4e1ef642c59598e8029d07066e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "train-00000-of-00001.parquet:   0%|          | 0.00/27.8M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9a19f32893f24aa19279d35b84f33da0"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "test-00000-of-00001.parquet:   0%|          | 0.00/4.66M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "010461721c1743eeb45df981f2ac085e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "validation-00000-of-00001.parquet:   0%|          | 0.00/4.58M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c646d06e9e41445694d810f5d264dd3e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Generating train split:   0%|          | 0/30000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f30b2629bb3a41c5a40807f0e64978f4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Generating test split:   0%|          | 0/5000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "968be5f1bae245e387ffd63ee837b6e5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Generating validation split:   0%|          | 0/5000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ddf3e381d436439c960f6fef0a362746"
      }
     },
     "metadata": {}
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": "ds",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-21T12:42:26.086509Z",
     "iopub.execute_input": "2024-12-21T12:42:26.087243Z",
     "iopub.status.idle": "2024-12-21T12:42:26.092597Z",
     "shell.execute_reply.started": "2024-12-21T12:42:26.087209Z",
     "shell.execute_reply": "2024-12-21T12:42:26.091747Z"
    }
   },
   "outputs": [
    {
     "execution_count": 4,
     "output_type": "execute_result",
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['Description', 'Patient', 'Doctor', 'question'],\n        num_rows: 30000\n    })\n    test: Dataset({\n        features: ['Description', 'Patient', 'Doctor', 'question'],\n        num_rows: 5000\n    })\n    validation: Dataset({\n        features: ['Description', 'Patient', 'Doctor', 'question'],\n        num_rows: 5000\n    })\n})"
     },
     "metadata": {}
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": "# Tokenize",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def tokenize_function(example):\n    # Construct the prompt using the `input` column\n    \n    # Tokenize input and output\n    example['input_ids'] = tokenizer(example['Patient'], padding=\"max_length\",  # Add padding\n        truncation=True,       \n        max_length=512,        \n        return_tensors=\"pt\").input_ids\n    example['labels'] = tokenizer(example['Doctor'], padding=\"max_length\",  # Add padding\n        truncation=True,       \n        max_length=512,        \n        return_tensors=\"pt\").input_ids\n    return example\n\ntokenized_datasets = ds.map(tokenize_function, batched=True,remove_columns=ds[\"train\"].column_names)\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-21T12:42:40.941508Z",
     "iopub.execute_input": "2024-12-21T12:42:40.941856Z",
     "iopub.status.idle": "2024-12-21T12:43:09.800610Z",
     "shell.execute_reply.started": "2024-12-21T12:42:40.941825Z",
     "shell.execute_reply": "2024-12-21T12:43:09.799471Z"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Map:   0%|          | 0/30000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "165acd53c4e84810852be7cffa834b12"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6599358a395749ef98cc611f5811af4e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ea0e38adde544c8e81ba309ab685cd82"
      }
     },
     "metadata": {}
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": "# Before LORA",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def print_number_of_trainable_model_parameters(model):\n    trainable_model_params = 0\n    all_model_params = 0\n    for _, param in model.named_parameters():\n        all_model_params += param.numel()\n        if param.requires_grad:\n            trainable_model_params += param.numel()\n    return f'trainable model parameters: {trainable_model_params}\\n \\\n            all model parameters: {all_model_params} \\n \\\n            percentage of trainable model parameters: {(trainable_model_params / all_model_params) * 100} %'\n\n\nprint(print_number_of_trainable_model_parameters(model))",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-21T12:43:53.378324Z",
     "iopub.execute_input": "2024-12-21T12:43:53.378616Z",
     "iopub.status.idle": "2024-12-21T12:43:53.386452Z",
     "shell.execute_reply.started": "2024-12-21T12:43:53.378593Z",
     "shell.execute_reply": "2024-12-21T12:43:53.385614Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "trainable model parameters: 1543714304\n             all model parameters: 1543714304 \n             percentage of trainable model parameters: 100.0 %\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": "for name, module in model.named_modules():\n    print(f\"Module Name: {name}\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-21T12:44:04.304957Z",
     "iopub.execute_input": "2024-12-21T12:44:04.305258Z",
     "iopub.status.idle": "2024-12-21T12:44:04.369451Z",
     "shell.execute_reply.started": "2024-12-21T12:44:04.305226Z",
     "shell.execute_reply": "2024-12-21T12:44:04.368789Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Module Name: \nModule Name: model\nModule Name: model.embed_tokens\nModule Name: model.layers\nModule Name: model.layers.0\nModule Name: model.layers.0.self_attn\nModule Name: model.layers.0.self_attn.q_proj\nModule Name: model.layers.0.self_attn.k_proj\nModule Name: model.layers.0.self_attn.v_proj\nModule Name: model.layers.0.self_attn.o_proj\nModule Name: model.layers.0.self_attn.rotary_emb\nModule Name: model.layers.0.mlp\nModule Name: model.layers.0.mlp.gate_proj\nModule Name: model.layers.0.mlp.up_proj\nModule Name: model.layers.0.mlp.down_proj\nModule Name: model.layers.0.mlp.act_fn\nModule Name: model.layers.0.input_layernorm\nModule Name: model.layers.0.post_attention_layernorm\nModule Name: model.layers.1\nModule Name: model.layers.1.self_attn\nModule Name: model.layers.1.self_attn.q_proj\nModule Name: model.layers.1.self_attn.k_proj\nModule Name: model.layers.1.self_attn.v_proj\nModule Name: model.layers.1.self_attn.o_proj\nModule Name: model.layers.1.self_attn.rotary_emb\nModule Name: model.layers.1.mlp\nModule Name: model.layers.1.mlp.gate_proj\nModule Name: model.layers.1.mlp.up_proj\nModule Name: model.layers.1.mlp.down_proj\nModule Name: model.layers.1.mlp.act_fn\nModule Name: model.layers.1.input_layernorm\nModule Name: model.layers.1.post_attention_layernorm\nModule Name: model.layers.2\nModule Name: model.layers.2.self_attn\nModule Name: model.layers.2.self_attn.q_proj\nModule Name: model.layers.2.self_attn.k_proj\nModule Name: model.layers.2.self_attn.v_proj\nModule Name: model.layers.2.self_attn.o_proj\nModule Name: model.layers.2.self_attn.rotary_emb\nModule Name: model.layers.2.mlp\nModule Name: model.layers.2.mlp.gate_proj\nModule Name: model.layers.2.mlp.up_proj\nModule Name: model.layers.2.mlp.down_proj\nModule Name: model.layers.2.mlp.act_fn\nModule Name: model.layers.2.input_layernorm\nModule Name: model.layers.2.post_attention_layernorm\nModule Name: model.layers.3\nModule Name: model.layers.3.self_attn\nModule Name: model.layers.3.self_attn.q_proj\nModule Name: model.layers.3.self_attn.k_proj\nModule Name: model.layers.3.self_attn.v_proj\nModule Name: model.layers.3.self_attn.o_proj\nModule Name: model.layers.3.self_attn.rotary_emb\nModule Name: model.layers.3.mlp\nModule Name: model.layers.3.mlp.gate_proj\nModule Name: model.layers.3.mlp.up_proj\nModule Name: model.layers.3.mlp.down_proj\nModule Name: model.layers.3.mlp.act_fn\nModule Name: model.layers.3.input_layernorm\nModule Name: model.layers.3.post_attention_layernorm\nModule Name: model.layers.4\nModule Name: model.layers.4.self_attn\nModule Name: model.layers.4.self_attn.q_proj\nModule Name: model.layers.4.self_attn.k_proj\nModule Name: model.layers.4.self_attn.v_proj\nModule Name: model.layers.4.self_attn.o_proj\nModule Name: model.layers.4.self_attn.rotary_emb\nModule Name: model.layers.4.mlp\nModule Name: model.layers.4.mlp.gate_proj\nModule Name: model.layers.4.mlp.up_proj\nModule Name: model.layers.4.mlp.down_proj\nModule Name: model.layers.4.mlp.act_fn\nModule Name: model.layers.4.input_layernorm\nModule Name: model.layers.4.post_attention_layernorm\nModule Name: model.layers.5\nModule Name: model.layers.5.self_attn\nModule Name: model.layers.5.self_attn.q_proj\nModule Name: model.layers.5.self_attn.k_proj\nModule Name: model.layers.5.self_attn.v_proj\nModule Name: model.layers.5.self_attn.o_proj\nModule Name: model.layers.5.self_attn.rotary_emb\nModule Name: model.layers.5.mlp\nModule Name: model.layers.5.mlp.gate_proj\nModule Name: model.layers.5.mlp.up_proj\nModule Name: model.layers.5.mlp.down_proj\nModule Name: model.layers.5.mlp.act_fn\nModule Name: model.layers.5.input_layernorm\nModule Name: model.layers.5.post_attention_layernorm\nModule Name: model.layers.6\nModule Name: model.layers.6.self_attn\nModule Name: model.layers.6.self_attn.q_proj\nModule Name: model.layers.6.self_attn.k_proj\nModule Name: model.layers.6.self_attn.v_proj\nModule Name: model.layers.6.self_attn.o_proj\nModule Name: model.layers.6.self_attn.rotary_emb\nModule Name: model.layers.6.mlp\nModule Name: model.layers.6.mlp.gate_proj\nModule Name: model.layers.6.mlp.up_proj\nModule Name: model.layers.6.mlp.down_proj\nModule Name: model.layers.6.mlp.act_fn\nModule Name: model.layers.6.input_layernorm\nModule Name: model.layers.6.post_attention_layernorm\nModule Name: model.layers.7\nModule Name: model.layers.7.self_attn\nModule Name: model.layers.7.self_attn.q_proj\nModule Name: model.layers.7.self_attn.k_proj\nModule Name: model.layers.7.self_attn.v_proj\nModule Name: model.layers.7.self_attn.o_proj\nModule Name: model.layers.7.self_attn.rotary_emb\nModule Name: model.layers.7.mlp\nModule Name: model.layers.7.mlp.gate_proj\nModule Name: model.layers.7.mlp.up_proj\nModule Name: model.layers.7.mlp.down_proj\nModule Name: model.layers.7.mlp.act_fn\nModule Name: model.layers.7.input_layernorm\nModule Name: model.layers.7.post_attention_layernorm\nModule Name: model.layers.8\nModule Name: model.layers.8.self_attn\nModule Name: model.layers.8.self_attn.q_proj\nModule Name: model.layers.8.self_attn.k_proj\nModule Name: model.layers.8.self_attn.v_proj\nModule Name: model.layers.8.self_attn.o_proj\nModule Name: model.layers.8.self_attn.rotary_emb\nModule Name: model.layers.8.mlp\nModule Name: model.layers.8.mlp.gate_proj\nModule Name: model.layers.8.mlp.up_proj\nModule Name: model.layers.8.mlp.down_proj\nModule Name: model.layers.8.mlp.act_fn\nModule Name: model.layers.8.input_layernorm\nModule Name: model.layers.8.post_attention_layernorm\nModule Name: model.layers.9\nModule Name: model.layers.9.self_attn\nModule Name: model.layers.9.self_attn.q_proj\nModule Name: model.layers.9.self_attn.k_proj\nModule Name: model.layers.9.self_attn.v_proj\nModule Name: model.layers.9.self_attn.o_proj\nModule Name: model.layers.9.self_attn.rotary_emb\nModule Name: model.layers.9.mlp\nModule Name: model.layers.9.mlp.gate_proj\nModule Name: model.layers.9.mlp.up_proj\nModule Name: model.layers.9.mlp.down_proj\nModule Name: model.layers.9.mlp.act_fn\nModule Name: model.layers.9.input_layernorm\nModule Name: model.layers.9.post_attention_layernorm\nModule Name: model.layers.10\nModule Name: model.layers.10.self_attn\nModule Name: model.layers.10.self_attn.q_proj\nModule Name: model.layers.10.self_attn.k_proj\nModule Name: model.layers.10.self_attn.v_proj\nModule Name: model.layers.10.self_attn.o_proj\nModule Name: model.layers.10.self_attn.rotary_emb\nModule Name: model.layers.10.mlp\nModule Name: model.layers.10.mlp.gate_proj\nModule Name: model.layers.10.mlp.up_proj\nModule Name: model.layers.10.mlp.down_proj\nModule Name: model.layers.10.mlp.act_fn\nModule Name: model.layers.10.input_layernorm\nModule Name: model.layers.10.post_attention_layernorm\nModule Name: model.layers.11\nModule Name: model.layers.11.self_attn\nModule Name: model.layers.11.self_attn.q_proj\nModule Name: model.layers.11.self_attn.k_proj\nModule Name: model.layers.11.self_attn.v_proj\nModule Name: model.layers.11.self_attn.o_proj\nModule Name: model.layers.11.self_attn.rotary_emb\nModule Name: model.layers.11.mlp\nModule Name: model.layers.11.mlp.gate_proj\nModule Name: model.layers.11.mlp.up_proj\nModule Name: model.layers.11.mlp.down_proj\nModule Name: model.layers.11.mlp.act_fn\nModule Name: model.layers.11.input_layernorm\nModule Name: model.layers.11.post_attention_layernorm\nModule Name: model.layers.12\nModule Name: model.layers.12.self_attn\nModule Name: model.layers.12.self_attn.q_proj\nModule Name: model.layers.12.self_attn.k_proj\nModule Name: model.layers.12.self_attn.v_proj\nModule Name: model.layers.12.self_attn.o_proj\nModule Name: model.layers.12.self_attn.rotary_emb\nModule Name: model.layers.12.mlp\nModule Name: model.layers.12.mlp.gate_proj\nModule Name: model.layers.12.mlp.up_proj\nModule Name: model.layers.12.mlp.down_proj\nModule Name: model.layers.12.mlp.act_fn\nModule Name: model.layers.12.input_layernorm\nModule Name: model.layers.12.post_attention_layernorm\nModule Name: model.layers.13\nModule Name: model.layers.13.self_attn\nModule Name: model.layers.13.self_attn.q_proj\nModule Name: model.layers.13.self_attn.k_proj\nModule Name: model.layers.13.self_attn.v_proj\nModule Name: model.layers.13.self_attn.o_proj\nModule Name: model.layers.13.self_attn.rotary_emb\nModule Name: model.layers.13.mlp\nModule Name: model.layers.13.mlp.gate_proj\nModule Name: model.layers.13.mlp.up_proj\nModule Name: model.layers.13.mlp.down_proj\nModule Name: model.layers.13.mlp.act_fn\nModule Name: model.layers.13.input_layernorm\nModule Name: model.layers.13.post_attention_layernorm\nModule Name: model.layers.14\nModule Name: model.layers.14.self_attn\nModule Name: model.layers.14.self_attn.q_proj\nModule Name: model.layers.14.self_attn.k_proj\nModule Name: model.layers.14.self_attn.v_proj\nModule Name: model.layers.14.self_attn.o_proj\nModule Name: model.layers.14.self_attn.rotary_emb\nModule Name: model.layers.14.mlp\nModule Name: model.layers.14.mlp.gate_proj\nModule Name: model.layers.14.mlp.up_proj\nModule Name: model.layers.14.mlp.down_proj\nModule Name: model.layers.14.mlp.act_fn\nModule Name: model.layers.14.input_layernorm\nModule Name: model.layers.14.post_attention_layernorm\nModule Name: model.layers.15\nModule Name: model.layers.15.self_attn\nModule Name: model.layers.15.self_attn.q_proj\nModule Name: model.layers.15.self_attn.k_proj\nModule Name: model.layers.15.self_attn.v_proj\nModule Name: model.layers.15.self_attn.o_proj\nModule Name: model.layers.15.self_attn.rotary_emb\nModule Name: model.layers.15.mlp\nModule Name: model.layers.15.mlp.gate_proj\nModule Name: model.layers.15.mlp.up_proj\nModule Name: model.layers.15.mlp.down_proj\nModule Name: model.layers.15.mlp.act_fn\nModule Name: model.layers.15.input_layernorm\nModule Name: model.layers.15.post_attention_layernorm\nModule Name: model.layers.16\nModule Name: model.layers.16.self_attn\nModule Name: model.layers.16.self_attn.q_proj\nModule Name: model.layers.16.self_attn.k_proj\nModule Name: model.layers.16.self_attn.v_proj\nModule Name: model.layers.16.self_attn.o_proj\nModule Name: model.layers.16.self_attn.rotary_emb\nModule Name: model.layers.16.mlp\nModule Name: model.layers.16.mlp.gate_proj\nModule Name: model.layers.16.mlp.up_proj\nModule Name: model.layers.16.mlp.down_proj\nModule Name: model.layers.16.mlp.act_fn\nModule Name: model.layers.16.input_layernorm\nModule Name: model.layers.16.post_attention_layernorm\nModule Name: model.layers.17\nModule Name: model.layers.17.self_attn\nModule Name: model.layers.17.self_attn.q_proj\nModule Name: model.layers.17.self_attn.k_proj\nModule Name: model.layers.17.self_attn.v_proj\nModule Name: model.layers.17.self_attn.o_proj\nModule Name: model.layers.17.self_attn.rotary_emb\nModule Name: model.layers.17.mlp\nModule Name: model.layers.17.mlp.gate_proj\nModule Name: model.layers.17.mlp.up_proj\nModule Name: model.layers.17.mlp.down_proj\nModule Name: model.layers.17.mlp.act_fn\nModule Name: model.layers.17.input_layernorm\nModule Name: model.layers.17.post_attention_layernorm\nModule Name: model.layers.18\nModule Name: model.layers.18.self_attn\nModule Name: model.layers.18.self_attn.q_proj\nModule Name: model.layers.18.self_attn.k_proj\nModule Name: model.layers.18.self_attn.v_proj\nModule Name: model.layers.18.self_attn.o_proj\nModule Name: model.layers.18.self_attn.rotary_emb\nModule Name: model.layers.18.mlp\nModule Name: model.layers.18.mlp.gate_proj\nModule Name: model.layers.18.mlp.up_proj\nModule Name: model.layers.18.mlp.down_proj\nModule Name: model.layers.18.mlp.act_fn\nModule Name: model.layers.18.input_layernorm\nModule Name: model.layers.18.post_attention_layernorm\nModule Name: model.layers.19\nModule Name: model.layers.19.self_attn\nModule Name: model.layers.19.self_attn.q_proj\nModule Name: model.layers.19.self_attn.k_proj\nModule Name: model.layers.19.self_attn.v_proj\nModule Name: model.layers.19.self_attn.o_proj\nModule Name: model.layers.19.self_attn.rotary_emb\nModule Name: model.layers.19.mlp\nModule Name: model.layers.19.mlp.gate_proj\nModule Name: model.layers.19.mlp.up_proj\nModule Name: model.layers.19.mlp.down_proj\nModule Name: model.layers.19.mlp.act_fn\nModule Name: model.layers.19.input_layernorm\nModule Name: model.layers.19.post_attention_layernorm\nModule Name: model.layers.20\nModule Name: model.layers.20.self_attn\nModule Name: model.layers.20.self_attn.q_proj\nModule Name: model.layers.20.self_attn.k_proj\nModule Name: model.layers.20.self_attn.v_proj\nModule Name: model.layers.20.self_attn.o_proj\nModule Name: model.layers.20.self_attn.rotary_emb\nModule Name: model.layers.20.mlp\nModule Name: model.layers.20.mlp.gate_proj\nModule Name: model.layers.20.mlp.up_proj\nModule Name: model.layers.20.mlp.down_proj\nModule Name: model.layers.20.mlp.act_fn\nModule Name: model.layers.20.input_layernorm\nModule Name: model.layers.20.post_attention_layernorm\nModule Name: model.layers.21\nModule Name: model.layers.21.self_attn\nModule Name: model.layers.21.self_attn.q_proj\nModule Name: model.layers.21.self_attn.k_proj\nModule Name: model.layers.21.self_attn.v_proj\nModule Name: model.layers.21.self_attn.o_proj\nModule Name: model.layers.21.self_attn.rotary_emb\nModule Name: model.layers.21.mlp\nModule Name: model.layers.21.mlp.gate_proj\nModule Name: model.layers.21.mlp.up_proj\nModule Name: model.layers.21.mlp.down_proj\nModule Name: model.layers.21.mlp.act_fn\nModule Name: model.layers.21.input_layernorm\nModule Name: model.layers.21.post_attention_layernorm\nModule Name: model.layers.22\nModule Name: model.layers.22.self_attn\nModule Name: model.layers.22.self_attn.q_proj\nModule Name: model.layers.22.self_attn.k_proj\nModule Name: model.layers.22.self_attn.v_proj\nModule Name: model.layers.22.self_attn.o_proj\nModule Name: model.layers.22.self_attn.rotary_emb\nModule Name: model.layers.22.mlp\nModule Name: model.layers.22.mlp.gate_proj\nModule Name: model.layers.22.mlp.up_proj\nModule Name: model.layers.22.mlp.down_proj\nModule Name: model.layers.22.mlp.act_fn\nModule Name: model.layers.22.input_layernorm\nModule Name: model.layers.22.post_attention_layernorm\nModule Name: model.layers.23\nModule Name: model.layers.23.self_attn\nModule Name: model.layers.23.self_attn.q_proj\nModule Name: model.layers.23.self_attn.k_proj\nModule Name: model.layers.23.self_attn.v_proj\nModule Name: model.layers.23.self_attn.o_proj\nModule Name: model.layers.23.self_attn.rotary_emb\nModule Name: model.layers.23.mlp\nModule Name: model.layers.23.mlp.gate_proj\nModule Name: model.layers.23.mlp.up_proj\nModule Name: model.layers.23.mlp.down_proj\nModule Name: model.layers.23.mlp.act_fn\nModule Name: model.layers.23.input_layernorm\nModule Name: model.layers.23.post_attention_layernorm\nModule Name: model.layers.24\nModule Name: model.layers.24.self_attn\nModule Name: model.layers.24.self_attn.q_proj\nModule Name: model.layers.24.self_attn.k_proj\nModule Name: model.layers.24.self_attn.v_proj\nModule Name: model.layers.24.self_attn.o_proj\nModule Name: model.layers.24.self_attn.rotary_emb\nModule Name: model.layers.24.mlp\nModule Name: model.layers.24.mlp.gate_proj\nModule Name: model.layers.24.mlp.up_proj\nModule Name: model.layers.24.mlp.down_proj\nModule Name: model.layers.24.mlp.act_fn\nModule Name: model.layers.24.input_layernorm\nModule Name: model.layers.24.post_attention_layernorm\nModule Name: model.layers.25\nModule Name: model.layers.25.self_attn\nModule Name: model.layers.25.self_attn.q_proj\nModule Name: model.layers.25.self_attn.k_proj\nModule Name: model.layers.25.self_attn.v_proj\nModule Name: model.layers.25.self_attn.o_proj\nModule Name: model.layers.25.self_attn.rotary_emb\nModule Name: model.layers.25.mlp\nModule Name: model.layers.25.mlp.gate_proj\nModule Name: model.layers.25.mlp.up_proj\nModule Name: model.layers.25.mlp.down_proj\nModule Name: model.layers.25.mlp.act_fn\nModule Name: model.layers.25.input_layernorm\nModule Name: model.layers.25.post_attention_layernorm\nModule Name: model.layers.26\nModule Name: model.layers.26.self_attn\nModule Name: model.layers.26.self_attn.q_proj\nModule Name: model.layers.26.self_attn.k_proj\nModule Name: model.layers.26.self_attn.v_proj\nModule Name: model.layers.26.self_attn.o_proj\nModule Name: model.layers.26.self_attn.rotary_emb\nModule Name: model.layers.26.mlp\nModule Name: model.layers.26.mlp.gate_proj\nModule Name: model.layers.26.mlp.up_proj\nModule Name: model.layers.26.mlp.down_proj\nModule Name: model.layers.26.mlp.act_fn\nModule Name: model.layers.26.input_layernorm\nModule Name: model.layers.26.post_attention_layernorm\nModule Name: model.layers.27\nModule Name: model.layers.27.self_attn\nModule Name: model.layers.27.self_attn.q_proj\nModule Name: model.layers.27.self_attn.k_proj\nModule Name: model.layers.27.self_attn.v_proj\nModule Name: model.layers.27.self_attn.o_proj\nModule Name: model.layers.27.self_attn.rotary_emb\nModule Name: model.layers.27.mlp\nModule Name: model.layers.27.mlp.gate_proj\nModule Name: model.layers.27.mlp.up_proj\nModule Name: model.layers.27.mlp.down_proj\nModule Name: model.layers.27.mlp.act_fn\nModule Name: model.layers.27.input_layernorm\nModule Name: model.layers.27.post_attention_layernorm\nModule Name: model.norm\nModule Name: model.rotary_emb\nModule Name: lm_head\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "source": "from peft import LoraConfig, get_peft_model, TaskType\n\npeft_config = LoraConfig(r=8,\n                         lora_alpha=32, \n                         target_modules=['q_proj','k_proj'], \n                         lora_dropout = 0.05,\n                         bias='none',\n                         task_type=TaskType.CAUSAL_LM \n)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-21T12:44:31.386926Z",
     "iopub.execute_input": "2024-12-21T12:44:31.387243Z",
     "iopub.status.idle": "2024-12-21T12:44:31.391353Z",
     "shell.execute_reply.started": "2024-12-21T12:44:31.387220Z",
     "shell.execute_reply": "2024-12-21T12:44:31.390509Z"
    }
   },
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": "# After LORA",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from peft import prepare_model_for_kbit_training\n\nmodel = prepare_model_for_kbit_training(model)\npeft_model = get_peft_model(model, peft_config)\n\nprint(print_number_of_trainable_model_parameters(peft_model))",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-21T12:44:32.516941Z",
     "iopub.execute_input": "2024-12-21T12:44:32.517240Z",
     "iopub.status.idle": "2024-12-21T12:44:32.826291Z",
     "shell.execute_reply.started": "2024-12-21T12:44:32.517219Z",
     "shell.execute_reply": "2024-12-21T12:44:32.825548Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "trainable model parameters: 1089536\n             all model parameters: 1544803840 \n             percentage of trainable model parameters: 0.07052908413277896 %\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": "# Train",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import time\noutput_dir = f'./chatbot-dialogue-training-{str(int(time.time()))}'\nfrom trl import SFTTrainer\n\ntotal_training_steps = 5000 \n\nlog_steps = 100\nsave_steps = 500\neval_steps = 100\n\n# TrainingArguments\nargs = TrainingArguments(\n    output_dir=output_dir,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,    \n    gradient_checkpointing=True,\n    gradient_accumulation_steps=1,\n    learning_rate=1e-5,\n    max_steps=3000,\n    weight_decay=0.01,\n    logging_steps=500,\n    save_strategy='steps',\n    evaluation_strategy=\"steps\",\n    report_to='none'\n)\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-21T14:12:58.743590Z",
     "iopub.execute_input": "2024-12-21T14:12:58.743924Z",
     "iopub.status.idle": "2024-12-21T14:12:58.771583Z",
     "shell.execute_reply.started": "2024-12-21T14:12:58.743895Z",
     "shell.execute_reply": "2024-12-21T14:12:58.770719Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "source": "# First, define the metric functions as you shared\ndef preprocess_logits_for_metrics(logits, labels):\n    pred_ids = torch.argmax(logits[0], dim=-1)\n    return pred_ids, labels",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-21T14:13:00.458716Z",
     "iopub.execute_input": "2024-12-21T14:13:00.459046Z",
     "iopub.status.idle": "2024-12-21T14:13:00.463237Z",
     "shell.execute_reply.started": "2024-12-21T14:13:00.459017Z",
     "shell.execute_reply": "2024-12-21T14:13:00.462426Z"
    }
   },
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "source": "trainer = SFTTrainer(\n    model=model,\n    peft_config=peft_config,\n    args=args,\n    train_dataset=tokenized_datasets['train'],# Train split\n    eval_dataset=tokenized_datasets['validation'],\n    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-21T14:13:01.395950Z",
     "iopub.execute_input": "2024-12-21T14:13:01.396269Z",
     "iopub.status.idle": "2024-12-21T14:13:02.069344Z",
     "shell.execute_reply.started": "2024-12-21T14:13:01.396244Z",
     "shell.execute_reply": "2024-12-21T14:13:02.068632Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "source": "trainer.train()",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-21T14:13:05.110878Z",
     "iopub.execute_input": "2024-12-21T14:13:05.111207Z",
     "iopub.status.idle": "2024-12-21T18:19:38.706356Z",
     "shell.execute_reply.started": "2024-12-21T14:13:05.111179Z",
     "shell.execute_reply": "2024-12-21T18:19:38.705624Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3000/3000 4:06:30, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>4.490800</td>\n      <td>0.702286</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.695700</td>\n      <td>0.669651</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.671600</td>\n      <td>0.661402</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.675000</td>\n      <td>0.656818</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.661300</td>\n      <td>0.654578</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.654200</td>\n      <td>0.653953</td>\n    </tr>\n  </tbody>\n</table><p>"
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
     "output_type": "stream"
    },
    {
     "execution_count": 37,
     "output_type": "execute_result",
     "data": {
      "text/plain": "TrainOutput(global_step=3000, training_loss=1.3081114501953126, metrics={'train_runtime': 14793.1525, 'train_samples_per_second': 0.811, 'train_steps_per_second': 0.203, 'total_flos': 4.8344560828416e+16, 'train_loss': 1.3081114501953126, 'epoch': 0.4})"
     },
     "metadata": {}
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "source": "trainer.push_to_hub()",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-21T18:48:59.891369Z",
     "iopub.execute_input": "2024-12-21T18:48:59.891714Z",
     "iopub.status.idle": "2024-12-21T18:49:02.218282Z",
     "shell.execute_reply.started": "2024-12-21T18:48:59.891686Z",
     "shell.execute_reply": "2024-12-21T18:49:02.217578Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "No files have been modified since last commit. Skipping to prevent empty commit.\n",
     "output_type": "stream"
    },
    {
     "execution_count": 46,
     "output_type": "execute_result",
     "data": {
      "text/plain": "CommitInfo(commit_url='https://huggingface.co/BoghdadyJR/chatbot-dialogue-training-1734790378/commit/404ee0690dff75387b81dce21dc2f54634b913e7', commit_message='End of training', commit_description='', oid='404ee0690dff75387b81dce21dc2f54634b913e7', pr_url=None, repo_url=RepoUrl('https://huggingface.co/BoghdadyJR/chatbot-dialogue-training-1734790378', endpoint='https://huggingface.co', repo_type='model', repo_id='BoghdadyJR/chatbot-dialogue-training-1734790378'), pr_revision=None, pr_num=None)"
     },
     "metadata": {}
    }
   ],
   "execution_count": 46
  },
  {
   "cell_type": "markdown",
   "source": "# Evaluate",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Load model directly\nfrom transformers import pipeline\ngenerator = pipeline(\"text-generation\", model=\"BoghdadyJR/chatbot-dialogue-training-1734790378\", device=\"cuda\")\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-21T18:56:48.349371Z",
     "iopub.execute_input": "2024-12-21T18:56:48.349672Z",
     "iopub.status.idle": "2024-12-21T18:56:55.188564Z",
     "shell.execute_reply.started": "2024-12-21T18:56:48.349648Z",
     "shell.execute_reply": "2024-12-21T18:56:55.187669Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "Device set to use cuda\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "source": "generator.model.push_to_hub(\"med\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-21T18:57:57.565463Z",
     "iopub.execute_input": "2024-12-21T18:57:57.565894Z",
     "iopub.status.idle": "2024-12-21T19:01:28.368543Z",
     "shell.execute_reply.started": "2024-12-21T18:57:57.565858Z",
     "shell.execute_reply": "2024-12-21T19:01:28.367747Z"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "494f535864f149878cb5b1a57e5bbd2b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "model-00002-of-00002.safetensors:   0%|          | 0.00/1.18G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a2078bd5074d4e998e54374ba8316d24"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5882eab01df14a7ea10039f899bb1ad2"
      }
     },
     "metadata": {}
    },
    {
     "execution_count": 51,
     "output_type": "execute_result",
     "data": {
      "text/plain": "CommitInfo(commit_url='https://huggingface.co/BoghdadyJR/med/commit/2a3ccf689417653477fe03cf9f8a977727edbe3d', commit_message='Upload Qwen2ForCausalLM', commit_description='', oid='2a3ccf689417653477fe03cf9f8a977727edbe3d', pr_url=None, repo_url=RepoUrl('https://huggingface.co/BoghdadyJR/med', endpoint='https://huggingface.co', repo_type='model', repo_id='BoghdadyJR/med'), pr_revision=None, pr_num=None)"
     },
     "metadata": {}
    }
   ],
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "source": "generator.tokenizer.push_to_hub(\"med\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-21T19:18:42.743662Z",
     "iopub.execute_input": "2024-12-21T19:18:42.743990Z",
     "iopub.status.idle": "2024-12-21T19:18:52.474630Z",
     "shell.execute_reply.started": "2024-12-21T19:18:42.743966Z",
     "shell.execute_reply": "2024-12-21T19:18:52.473896Z"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "667d5e618a654db6bf78a869fc85caa8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7f164021ce2c4b1bb9ce2e2b196ecf90"
      }
     },
     "metadata": {}
    },
    {
     "execution_count": 53,
     "output_type": "execute_result",
     "data": {
      "text/plain": "CommitInfo(commit_url='https://huggingface.co/BoghdadyJR/med/commit/6c8ce0c4f95850a279e9b08949199ef40f864164', commit_message='Upload tokenizer', commit_description='', oid='6c8ce0c4f95850a279e9b08949199ef40f864164', pr_url=None, repo_url=RepoUrl('https://huggingface.co/BoghdadyJR/med', endpoint='https://huggingface.co', repo_type='model', repo_id='BoghdadyJR/med'), pr_revision=None, pr_num=None)"
     },
     "metadata": {}
    }
   ],
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "source": "model=generator.model\ntokenizer=generator.tokenizer",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-21T19:43:17.586124Z",
     "iopub.execute_input": "2024-12-21T19:43:17.586437Z",
     "iopub.status.idle": "2024-12-21T19:43:17.595760Z",
     "shell.execute_reply.started": "2024-12-21T19:43:17.586413Z",
     "shell.execute_reply": "2024-12-21T19:43:17.594884Z"
    }
   },
   "outputs": [],
   "execution_count": 60
  },
  {
   "cell_type": "code",
   "source": "test_samples = ds[\"test\"].shuffle().select(range(20))\ntest_samples",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-21T20:14:51.312158Z",
     "iopub.execute_input": "2024-12-21T20:14:51.312473Z",
     "iopub.status.idle": "2024-12-21T20:14:51.331987Z",
     "shell.execute_reply.started": "2024-12-21T20:14:51.312446Z",
     "shell.execute_reply": "2024-12-21T20:14:51.331117Z"
    }
   },
   "outputs": [
    {
     "execution_count": 67,
     "output_type": "execute_result",
     "data": {
      "text/plain": "Dataset({\n    features: ['Description', 'Patient', 'Doctor', 'question'],\n    num_rows: 20\n})"
     },
     "metadata": {}
    }
   ],
   "execution_count": 67
  },
  {
   "cell_type": "code",
   "source": "from tqdm import tqdm\nimport torch\nimport numpy as np\nfrom rouge_score import rouge_scorer\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n\n# Check if CUDA is available and set the device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Move model to the device\nmodel = model.to(device)\n\n# Initialize ROUGE scorer\nrouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n\n# Smoothing function for BLEU\nsmooth_fn = SmoothingFunction().method4\n\n# Function to generate predictions\ndef generate_prediction(input_text, model, tokenizer):\n    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=512)\n    inputs = {key: value.to(device) for key, value in inputs.items()}  # Move inputs to the same device\n    outputs = model.generate(**inputs, max_new_tokens=50, num_beams=5)  # Use max_new_tokens for better handling\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Perplexity calculation\ndef calculate_perplexity(prediction, model, tokenizer):\n    inputs = tokenizer(prediction, return_tensors=\"pt\", truncation=True, max_length=512)\n    inputs = {key: value.to(device) for key, value in inputs.items()}  # Move inputs to the same device\n    with torch.no_grad():\n        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n        loss = outputs.loss\n    return torch.exp(loss).item()\n\n# Select 20 samples from the dataset\ntest_samples = ds[\"test\"].shuffle().select(range(1000))\n\n# Evaluation metrics\nrouge_scores = []\nbleu_scores = []\nperplexity_scores = []\n\n# Iterate through the selected dataset with tqdm for progress tracking\nfor example in tqdm(test_samples, desc=\"Evaluating 20 Samples\"):\n    patient_input = example[\"Patient\"]\n    reference = example[\"Doctor\"]\n    \n    # Generate prediction\n    prediction = generate_prediction(patient_input, model, tokenizer)\n    \n    # Calculate ROUGE, BLEU, and Perplexity\n    scores = rouge.score(reference, prediction)\n    rouge_scores.append(scores)\n    \n    reference_tokens = reference.split()\n    prediction_tokens = prediction.split()\n    bleu_scores.append(sentence_bleu([reference_tokens], prediction_tokens, smoothing_function=smooth_fn))\n    \n    perplexity_scores.append(calculate_perplexity(prediction, model, tokenizer))\n\n# Calculate averages\naverage_rouge = {metric: np.mean([score[metric].fmeasure for score in rouge_scores]) for metric in ['rouge1', 'rouge2', 'rougeL']}\naverage_bleu = np.mean(bleu_scores)\naverage_perplexity = np.mean(perplexity_scores)\n\n# Print results\nprint(\"\\nEvaluation Results on 20 Samples:\")\nprint(f\"Final Average ROUGE Scores: {average_rouge}\")\nprint(f\"Final Average BLEU Score: {average_bleu}\")\nprint(f\"Final Average Perplexity: {average_perplexity}\")\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-21T20:35:17.880485Z",
     "iopub.execute_input": "2024-12-21T20:35:17.880850Z",
     "iopub.status.idle": "2024-12-21T21:10:32.218159Z",
     "shell.execute_reply.started": "2024-12-21T20:35:17.880792Z",
     "shell.execute_reply": "2024-12-21T21:10:32.217336Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Evaluating 20 Samples: 100%|██████████| 1000/1000 [35:14<00:00,  2.11s/it]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\nEvaluation Results on 20 Samples:\nFinal Average ROUGE Scores: {'rouge1': 0.2267385387469335, 'rouge2': 0.025187516285403107, 'rougeL': 0.11651081621718712}\nFinal Average BLEU Score: 0.13466384466782538\nFinal Average Perplexity: 9.996193381547927\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 73
  },
  {
   "cell_type": "code",
   "source": "print(\"\\nEvaluation Results on 1000 Samples:\")\nprint(f\"Final Average ROUGE Scores: {average_rouge}\")\nprint(f\"Final Average BLEU Score: {average_bleu}\")\nprint(f\"Final Average Perplexity: {average_perplexity}\")\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-12-21T21:10:52.023530Z",
     "iopub.execute_input": "2024-12-21T21:10:52.023846Z",
     "iopub.status.idle": "2024-12-21T21:10:52.028411Z",
     "shell.execute_reply.started": "2024-12-21T21:10:52.023819Z",
     "shell.execute_reply": "2024-12-21T21:10:52.027495Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "\nEvaluation Results on 1000 Samples:\nFinal Average ROUGE Scores: {'rouge1': 0.2267385387469335, 'rouge2': 0.025187516285403107, 'rougeL': 0.11651081621718712}\nFinal Average BLEU Score: 0.13466384466782538\nFinal Average Perplexity: 9.996193381547927\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 74
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
